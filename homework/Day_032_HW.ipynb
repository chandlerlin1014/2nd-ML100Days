{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 練習時間\n",
    "相信大家目前都能夠初步了解機器學習專案的流程及步驟，今天的作業希望大家能夠看看全球機器學習巨頭們在做的機器學習專案。以 google 為例，下圖是 Google 內部專案使用機器學習的數量，隨著時間進展，現在早已超過 2000 個專案在使用 ML。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://cdn-images-1.medium.com/max/800/1*U_L8qI8RmYS-MOBrYvXhSA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "底下幫同學整理幾間知名企業的 blog 或機器學習網站 (自行搜尋也可)，請挑選一篇文章閱讀並試著回答\n",
    "1. 專案的目標？ (要解決什麼問題）\n",
    "2. 使用的技術是？ (只需知道名稱即可，例如：使用 CNN 卷積神經網路做影像分類)\n",
    "3. 資料來源？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Google AI blog](https://ai.googleblog.com/)\n",
    "- [Facebook Research blog](https://research.fb.com/blog/)\n",
    "- [Apple machine learning journal](https://machinelearning.apple.com/)\n",
    "- [機器之心](https://www.jiqizhixin.com/)\n",
    "- [雷鋒網](http://www.leiphone.com/category/ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n專案的目標？ (要解決什麼問題）\\nAn ML Pipeline for Hand Tracking and Gesture Recognition\\n\\n使用的技術是？ (只需知道名稱即可，例如：使用 CNN 卷積神經網路做影像分類)\\nOur solution addresses the above challenges using different strategies. \\nFirst, we train a palm detector instead of a hand detector, since estimating bounding boxes of rigid objects like palms and fists is significantly simpler than detecting hands with articulated fingers. \\nIn addition, as palms are smaller objects, the non-maximum suppression algorithm works well even for two-hand self-occlusion cases, like handshakes. Moreover, palms can be modelled using square bounding boxes (anchors in ML terminology) ignoring other aspect ratios, and therefore reducing the number of anchors by a factor of 3-5. \\nSecond, an encoder-decoder feature extractor is used for bigger scene context awareness even for small objects (similar to the RetinaNet approach). \\nLastly, we minimize the focal loss during training to support a large amount of anchors resulting from the high scale variance. \\n\\n資料來源？\\nTo obtain ground truth data, we have manually annotated ~30K real-world images with 21 3D coordinates, as shown below (we take Z-value from image depth map, if it exists per corresponding coordinate). \\nTo better cover the possible hand poses and provide additional supervision on the nature of hand geometry, we also render a high-quality synthetic hand model over various backgrounds and map it to the corresponding 3D coordinates.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 挑選 Google AI blog的 On-Device, Real-Time Hand Tracking with MediaPipe\n",
    "\"\"\"\n",
    "專案的目標？ (要解決什麼問題）\n",
    "An ML Pipeline for Hand Tracking and Gesture Recognition\n",
    "\n",
    "使用的技術是？ (只需知道名稱即可，例如：使用 CNN 卷積神經網路做影像分類)\n",
    "Our solution addresses the above challenges using different strategies. \n",
    "First, we train a palm detector instead of a hand detector, since estimating bounding boxes of rigid objects like palms and fists is significantly simpler than detecting hands with articulated fingers. \n",
    "In addition, as palms are smaller objects, the non-maximum suppression algorithm works well even for two-hand self-occlusion cases, like handshakes. Moreover, palms can be modelled using square bounding boxes (anchors in ML terminology) ignoring other aspect ratios, and therefore reducing the number of anchors by a factor of 3-5. \n",
    "Second, an encoder-decoder feature extractor is used for bigger scene context awareness even for small objects (similar to the RetinaNet approach). \n",
    "Lastly, we minimize the focal loss during training to support a large amount of anchors resulting from the high scale variance. \n",
    "\n",
    "資料來源？\n",
    "To obtain ground truth data, we have manually annotated ~30K real-world images with 21 3D coordinates, as shown below (we take Z-value from image depth map, if it exists per corresponding coordinate). \n",
    "To better cover the possible hand poses and provide additional supervision on the nature of hand geometry, we also render a high-quality synthetic hand model over various backgrounds and map it to the corresponding 3D coordinates.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
